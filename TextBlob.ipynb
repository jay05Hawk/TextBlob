{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN5jlJssMv2QebLAsxp+vKo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jay05Hawk/TextBlob/blob/main/TextBlob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{TextBlob:}}$ Simplified Text Processing \n",
        "TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation,and more.\n",
        "\n",
        "TextBlob stands on the giant shoulders of NLTK and pattern, and plays nicely with both.\n",
        "\n",
        "$\\color{red}{\\text{Features:}}$\n",
        "- Noun phrase extraction\n",
        "- Part-of-speech tagging\n",
        "- Sentiment analysis\n",
        "- Classification (Naive Bayes, Decision Tree)\n",
        "- Language translation and detection powered by Google Translate\n",
        "- Tokenization (splitting text into words and sentences)\n",
        "- Word and phrase frequencies\n",
        "- Parsing\n",
        "- n-grams\n",
        "- Word inflection (pluralization and singularization) and lemmatization\n",
        "- Spelling correction\n",
        "- Add new models or languages through extensions\n",
        "- WordNet integration"
      ],
      "metadata": {
        "id": "wzLzSFulhFz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GFgk-LnhBYB"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#$\\color{red}{\\text{Let's create our first TextBlob}}$"
      ],
      "metadata": {
        "id": "mhl4TteMiNep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki = TextBlob(\"I love Natural Language Processing, not you!\")"
      ],
      "metadata": {
        "id": "gMkQnQoXiTG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\color{red}{\\text{Part-of-speech(POS) Tagging}}$"
      ],
      "metadata": {
        "id": "dj5gv5aKinbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wiki.tags"
      ],
      "metadata": {
        "id": "vSmYaycsiTKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install TextBlob\n",
        "from textblob import TextBlob, Word, Blobber\n",
        "#wiki.noun_phrases"
      ],
      "metadata": {
        "id": "ERfrVWrBiTNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "_v9q1qsliTP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\color{red}{\\text{Noun Phrase Extraction}}$ \n",
        "\n",
        "Similarly, noun phrases are accessed through the **noun_phrases** property."
      ],
      "metadata": {
        "id": "Dn7pTRMolWFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki.noun_phrases"
      ],
      "metadata": {
        "id": "hXO8VJDMiTSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\color{red}{\\text{Sentiment Analysis}}$ \n",
        "\n",
        "The sentiment property returns a named tuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
      ],
      "metadata": {
        "id": "y5XDErESlzjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testimonial = TextBlob(\"Textblob is amazingly simple to use. What great fun!\")\n",
        "testimonial.sentiment"
      ],
      "metadata": {
        "id": "D02xIAzwiTl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimonial.sentiment.subjectivity"
      ],
      "metadata": {
        "id": "gxHgGIGsiTpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\color{red}{\\text{Tokenization}}$\n"
      ],
      "metadata": {
        "id": "JFDQL3rDmJyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zen = TextBlob(\"Data is a new fuel. \"\n",
        "               \"Explicit is better than implicit. \"\n",
        "               \"Simple is better than complex. \")\n",
        "               \n",
        "zen.words"
      ],
      "metadata": {
        "id": "iSI94WGPiTsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zen.sentences"
      ],
      "metadata": {
        "id": "QI4ZgYfOiTvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in zen.sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "enPxM_w_meM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\color{red}{\\text{Word Inflection and lemmatization}}$\n",
        "\n",
        "Each word in the **TextBlob.words** or **Sentence.words** is a **Word** object(a subclass of unicode) with useful methods, e.g. for word inflection."
      ],
      "metadata": {
        "id": "4qN_1zudmwBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = TextBlob('Use 4 spaces per indentation level')\n",
        "\n",
        "sentence.words"
      ],
      "metadata": {
        "id": "UVB_A3zjmeyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words[2].singularize()"
      ],
      "metadata": {
        "id": "kWMSxb-qme1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence.words[0].pluralize()"
      ],
      "metadata": {
        "id": "QT5QuvJlme4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words can be lemmatized just by calling the **$\\color{red}{\\text{lemmatize}}$** method."
      ],
      "metadata": {
        "id": "EUORnBiQnQy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "\n",
        "q = Word('lions')\n",
        "q.lemmatize()"
      ],
      "metadata": {
        "id": "SGOPpaDUme7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = Word(\"went\")\n",
        "q.lemmatize(\"v\") #Pass in WordNet part of speech (verb)"
      ],
      "metadata": {
        "id": "kYwmBqkBme-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\color{red}{\\text{WordNet Integeration}}$\n",
        "\n",
        "You can access the synets for a **Word** via the **synsets** property or the **get_synsets** method optionally passing in a parts-of-speech.\n",
        "\n",
        "### WordNet \n",
        "\n",
        "   WordNet is a lexical database that is dictionary for the English language, it is specifically for the natural language     processing.\n",
        "### Synset\n",
        "\n",
        "   It is a special kind of a simple interface that is present in the NLTK for look up words in WordNet. Synset instances are    the groupings of synonymous that express the same type of concept. Some words have only one synset and some have several."
      ],
      "metadata": {
        "id": "N1uAxzTFnwDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "from textblob.wordnet import VERB\n",
        "word = Word(\"goat\")\n",
        "word.synsets"
      ],
      "metadata": {
        "id": "XB6mNexrmfCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word(\"hack\").get_synsets(pos=VERB)"
      ],
      "metadata": {
        "id": "5S4QDblBmfSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access the definitions for each synset via the **definitions** property or the **define()** method, which can also take an optional part-of-speech(pos) argument."
      ],
      "metadata": {
        "id": "VkCvjc13pOxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Word(\"length\").definitions"
      ],
      "metadata": {
        "id": "949UO6fmmfWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word(\"corpus\").definitions"
      ],
      "metadata": {
        "id": "_KMsdj_amfZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also create $\\color{blue}{\\text{synsets}}$ directly."
      ],
      "metadata": {
        "id": "ywDIoAz6pg7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob.wordnet import Synset\n",
        "octopus = Synset('octopus.n.02')\n",
        "shrimp = Synset('shrimp.n.03')\n",
        "octopus.path_similarity(shrimp)"
      ],
      "metadata": {
        "id": "smh76TZTmfb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###$\\color{red}{\\text{WordLists}}$ \n",
        "\n",
        "A wordlist is just the Python list with additional methods.\n",
        "\n",
        "WordLists will find it out the words which are in the sentence and ignore the spaces in between them. "
      ],
      "metadata": {
        "id": "J3KsP2ARqCk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "animals = TextBlob(\"cow sheep octopus\")\n",
        "animals.words"
      ],
      "metadata": {
        "id": "QqyUanaEmfe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "animals.words.pluralize() # It'll pluralize the words"
      ],
      "metadata": {
        "id": "wDQk4TVPmf0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\color{red}{\\text{Spelling Correction}}$ \n",
        "\n",
        "For correcting the words you can use **$\\color{blue}{\\text{correct()}}$** method to attempt spelling correction."
      ],
      "metadata": {
        "id": "hmhAWc0TqZek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = TextBlob('can yyou pronounce thankk?')\n",
        "print(g.correct())"
      ],
      "metadata": {
        "id": "zCF4ij0imf2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word objects have a **spellcheck() Word.spellcheck()** , this method that returns a list of (word, confidence) tuples with spelling suggestions."
      ],
      "metadata": {
        "id": "C9NUVXTCrCEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "k = Word('longituode')\n",
        "k.spellcheck()"
      ],
      "metadata": {
        "id": "beI6qMygmf5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Word and Noun Phrase Frequencies\n",
        "\n",
        "There are two ways to get the frequency of a word or noun phrase in the **TextBlob**\n",
        "\n",
        "The first one is through the word_counts dictionary."
      ],
      "metadata": {
        "id": "7Ssq611arfHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = TextBlob('She sales sea shells at the sea shore.')\n",
        "\n",
        "sent.word_counts['sea']"
      ],
      "metadata": {
        "id": "CnwESoswmf8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent.words.count('Sea', case_sensitive=True) #You can specify whether or not the search should be case-sensitive (default is False)."
      ],
      "metadata": {
        "id": "zs8R_YVHmf_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## $\\color{red}{\\text{Translation and Language Detection}}$\n",
        "\n",
        "TextBlobs can be translated between languages."
      ],
      "metadata": {
        "id": "rvvUHgSvsF1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "blob = TextBlob(u\"My mama always said life was like a box of chocolates .\")\n",
        "blob.translate(from_lang=\"en\",to='hi')\n"
      ],
      "metadata": {
        "id": "PLHUj4lCr-Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chinese_blob = TextBlob(u\"有总比没有好 杰马塔迪\")\n",
        "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
      ],
      "metadata": {
        "id": "xyXShcY5r-Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = TextBlob(\"bonjour\")\n",
        "b.detect_language()"
      ],
      "metadata": {
        "id": "pGqqn2djxJ9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"कुछ नहीं से कुछ भला\")\n",
        "\n",
        "print(blob.detect_language())"
      ],
      "metadata": {
        "id": "7hIL0ja6r-L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing\n",
        "\n",
        "Use the **parse()** method to parse the text."
      ],
      "metadata": {
        "id": "dX8pSBYCxvWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = TextBlob(\"And now for something completely different.\")\n",
        "print(b.parse())"
      ],
      "metadata": {
        "id": "vSzitx0QxU9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zen[0:15]"
      ],
      "metadata": {
        "id": "Ica49T_nxVBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zen.upper()#We can use it as common string method."
      ],
      "metadata": {
        "id": "aR5bqbd1xVE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zen.find('than') #It shows that 'than' word starts from 39th place."
      ],
      "metadata": {
        "id": "WXEiAO5Cr-PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can make comparisons between TextBlobs and strings.\n",
        "a_blob = TextBlob('apple')\n",
        "s_blob = TextBlob('samsumg')\n",
        "\n",
        "a_blob < s_blob"
      ],
      "metadata": {
        "id": "TjK6Hclxr-Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can concatenate and interpolate TextBlobs and strings.\n",
        "\n",
        "a_blob + ' and ' + s_blob"
      ],
      "metadata": {
        "id": "5ZVZ8vztr-Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $\\color{red}{\\text{N-Grams}}$\n",
        "\n",
        "The **TextBlob.ngrams()** method returns a list of tuples of n successive words."
      ],
      "metadata": {
        "id": "prQdi6uDyn05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"Now is better than never.\")\n",
        "blob.ngrams(n=3)"
      ],
      "metadata": {
        "id": "c86TBoU_r-Zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Start and End Indices of Sentences\n",
        "\n",
        "Use sentence.start and sentence.end to get the indices where a sentence starts and ends within a **TextBlob.**"
      ],
      "metadata": {
        "id": "isAcXj5hy4sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for k in zen.sentences:\n",
        "    print(k)\n",
        "    print(\"---- Starts at index {}, Ends at index {}\".format(k.start, k.end))"
      ],
      "metadata": {
        "id": "1h2Ce4bgypoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# $\\color{red}{\\text{Let's start building the Text Classification system}}$\n",
        "\n",
        "The __textblob.classifiers__ module makes it simple to create custom classifiers.\n",
        "\n",
        "As an example, let’s create a custom sentiment analyzer.\n",
        "\n",
        "## Loading Data and Creating a Classifier\n",
        "\n",
        "First we’ll create some training and test data."
      ],
      "metadata": {
        "id": "xySFAx6QzLcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = [\n",
        "     ('I love this sandwich.', 'pos'),\n",
        "     ('this is an amazing place!', 'pos'),\n",
        "     ('I feel very good about these beers.', 'pos'),\n",
        "     ('this is my best work.', 'pos'),\n",
        "     (\"what an awesome view\", 'pos'),\n",
        "     ('I do not like this restaurant', 'neg'),\n",
        "     ('I am tired of this stuff.', 'neg'),\n",
        "     (\"I can't deal with this\", 'neg'),\n",
        "     ('he is my sworn enemy!', 'neg'),\n",
        "     ('my boss is horrible.', 'neg')\n",
        "]\n",
        "\n",
        "test = [\n",
        "     ('the beer was good.', 'pos'),\n",
        "     ('I do not enjoy my job', 'neg'),\n",
        "     (\"I ain't feeling dandy today.\", 'neg'),\n",
        "     (\"I feel amazing!\", 'pos'),\n",
        "     ('Gary is a friend of mine.', 'pos'),\n",
        "     (\"I can't believe I'm doing this.\", 'neg')\n",
        "]"
      ],
      "metadata": {
        "id": "o5rbeXGXypuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we’ll create a Naive Bayes classifier, passing the training data into the constructor.\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "cl = NaiveBayesClassifier(train)"
      ],
      "metadata": {
        "id": "45xUZrlLypzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data from Files\n",
        "\n",
        "You can also load data from common file formats including CSV, JSON, and TSV.\n",
        "\n",
        "CSV files should be formatted like so:\n",
        "\n",
        "      I love this sandwich.,pos\n",
        "      This is an amazing place!,pos\n",
        "      I do not like this restaurant,neg\n",
        "      \n",
        "JSON files should be formatted like so:\n",
        "\n",
        "[\n",
        "    {\"text\": \"I love this sandwich.\", \"label\": \"pos\"},\n",
        "    {\"text\": \"This is an amazing place!\", \"label\": \"pos\"},\n",
        "    {\"text\": \"I do not like this restaurant\", \"label\": \"neg\"}\n",
        "]\n",
        "\n",
        "You can then pass the opened file into the constructor."
      ],
      "metadata": {
        "id": "SBHvsOBZ2zVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('train.json', 'r') as fp:\n",
        "#     cl = NaiveBayesClassifier(fp, format=\"json\")"
      ],
      "metadata": {
        "id": "Km09uBS6yp2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classifying Text ---Call the *classify(text)* method to use the classifier.\n",
        "\n",
        "\n",
        "cl.classify(\"This is an amazing library!\")"
      ],
      "metadata": {
        "id": "IkZX2Bt7yp5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You can get the label probability distribution with the *prob_classify(text)* method.\n",
        "\n",
        "prob_dist = cl.prob_classify(\"I am suffering from cough and cold.\")\n",
        "prob_dist.max()"
      ],
      "metadata": {
        "id": "VHiWGoQXyp85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(prob_dist.prob(\"neg\"), 2)"
      ],
      "metadata": {
        "id": "_g91CVIGyqW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round(prob_dist.prob(\"pos\"), 2)"
      ],
      "metadata": {
        "id": "0QnmghIJyqaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying TextBlobs\n",
        "\n",
        "Another way to classify text is to pass a classifier into the constructor of TextBlob and call its *classify()* method."
      ],
      "metadata": {
        "id": "GdyohGAV4rVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "#blob = TextBlob(\"Alcohol is good. But the hangover is horrible.\", classifier=cl)\n",
        "blob = TextBlob(\"Gun is best for safety. but its danger to shoot.\", classifier=cl)\n",
        "blob.classify()"
      ],
      "metadata": {
        "id": "LXvLO_6Z1fWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The advantage of this approach is that you can classify sentences within a **TextBlob**.\n",
        "\n",
        "for b in blob.sentences:\n",
        "    print(b)\n",
        "    print(b.classify())"
      ],
      "metadata": {
        "id": "eG5hs_yi1faE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating Classifiers\n",
        "\n",
        "To compute the accuracy on our test set, use the **accuracy(test_data)** method."
      ],
      "metadata": {
        "id": "b-JvsYxJ5up7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cl.accuracy(test)"
      ],
      "metadata": {
        "id": "hFEH2bbE1fp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the show_informative_features() method to display a listing of the most informative features.\n",
        "\n",
        "cl.show_informative_features(5)"
      ],
      "metadata": {
        "id": "WIboj7tc1fsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Updating Classifiers with New Data\n",
        "\n",
        "Use the update(new_data) method to update a classifier with new training data."
      ],
      "metadata": {
        "id": "u-Rsw0e46Fou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = [('She is my best friend.', 'pos'),\n",
        "           (\"I'm happy to have a new friend.\", 'pos'),\n",
        "           (\"Stay thirsty, my friend.\", 'pos'),             \n",
        "           (\"He ain't from around here.\", 'neg')]\n",
        "\n",
        "cl.update(new_data)"
      ],
      "metadata": {
        "id": "s3T-VAGr1fvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl.accuracy(test)"
      ],
      "metadata": {
        "id": "_tdjiUi51jIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Extractors\n",
        "\n",
        "By default, the *NaiveBayesClassifier* uses a simple feature extractor that indicates which words in the training set are contained in a document.\n",
        "\n",
        "For example, the sentence “I love” might have the features contains(love): True or contains(hate): False.\n",
        "\n",
        "You can override this feature extractor by writing your own. A feature extractor is simply a function with document (the text to extract features from) as the first argument. The function may include a second argument, train_set (the training dataset), if necessary.\n",
        "\n",
        "The function should return a dictionary of features for document.\n",
        "\n",
        "For example, let’s create a feature extractor that just uses the first and last words of a document as its features."
      ],
      "metadata": {
        "id": "Tu329fHP6cFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def end_word_extractor(document):\n",
        "    tokens = document.split()\n",
        "    first_word, last_word = tokens[0], tokens[-1]\n",
        "    feats = {}\n",
        "    feats[\"first({0})\".format(first_word)] = True\n",
        "    feats[\"last({0})\".format(last_word)] = False\n",
        "    return feats"
      ],
      "metadata": {
        "id": "cc1Hbzh91jM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = end_word_extractor(\"I love\")"
      ],
      "metadata": {
        "id": "atRLfhz81jQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert features == {'last(love)': False, 'first(I)': True}"
      ],
      "metadata": {
        "id": "ksDifaxX1jpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We can then use the feature extractor in a classifier by passing it as the second argument of the constructor.\n",
        "\n",
        "cl2 = NaiveBayesClassifier(test, feature_extractor=end_word_extractor)"
      ],
      "metadata": {
        "id": "TvZIfJ491jsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(\"I'm excited to try my new classifier.\", classifier=cl2)\n",
        "blob.classify()"
      ],
      "metadata": {
        "id": "JVAPbSHv66SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZamKn1P68rg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}